{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d105d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da3f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gridworld environment\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid = np.array([\n",
    "            [0, 0, 0, 1],  # Goal at (0, 3)\n",
    "            [0, -1, 0, 0],  # Wall with reward -1\n",
    "            [0, 0, 0, 0],\n",
    "            [0, 0, 0, 0]  # Start at (3, 0)\n",
    "        ])\n",
    "        self.start_state = (3, 0)\n",
    "        self.state = self.start_state\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return self.grid[state] == 1 or self.grid[state] == -1\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        next_state = list(state)\n",
    "        if action == 0:  # Move up\n",
    "            next_state[0] = max(0, state[0] - 1)\n",
    "        elif action == 1:  # Move right\n",
    "            next_state[1] = min(3, state[1] + 1)\n",
    "        elif action == 2:  # Move down\n",
    "            next_state[0] = min(3, state[0] + 1)\n",
    "        elif action == 3:  # Move left\n",
    "            next_state[1] = max(0, state[1] - 1)\n",
    "        return tuple(next_state)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.get_next_state(self.state, action)\n",
    "        reward = self.grid[next_state]\n",
    "        self.state = next_state\n",
    "        done = self.is_terminal(next_state)\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6122a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        self.q_table = np.zeros((4, 4, 4))  # Q-values for each state-action pair\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            return random.randint(0, 3)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        max_future_q = np.max(self.q_table[next_state])  # Best Q-value for next state\n",
    "        current_q = self.q_table[state][action]\n",
    "        # Q-learning formula\n",
    "        self.q_table[state][action] = current_q + self.learning_rate * (\n",
    "            reward + self.discount_factor * max_future_q - current_q\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904c374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "episodes = 1000  # Number of training episodes\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()  # Reset the environment at the start of each episode\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)  # Choose an action\n",
    "        next_state, reward, done = env.step(action)  # Take the action and observe next state, reward\n",
    "        agent.update_q_value(state, action, reward, next_state)  # Update Q-values\n",
    "        state = next_state  # Move to the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3916ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-values:\n",
      "[[[ 0.6302412   0.81        0.57620767  0.63392118]\n",
      "  [ 0.64222494  0.9        -0.9282102   0.56937927]\n",
      "  [ 0.84766518  1.          0.74311535  0.71151853]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.729      -0.92023356  0.53814665  0.45015541]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.89984312  0.0252      0.0024152  -0.271     ]\n",
      "  [ 0.271       0.          0.          0.08095951]]\n",
      "\n",
      " [[ 0.6561      0.49508585  0.50325139  0.56107133]\n",
      "  [-0.271       0.70388531  0.01679606  0.059049  ]\n",
      "  [ 0.80417706  0.          0.01635983  0.07722434]\n",
      "  [ 0.0171      0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.59049     0.31368282  0.4815671   0.42865183]\n",
      "  [ 0.51127169  0.          0.          0.        ]\n",
      "  [ 0.18534253  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "State: (2, 0), Action: 0, Reward: 0\n",
      "State: (1, 0), Action: 0, Reward: 0\n",
      "State: (0, 0), Action: 0, Reward: 0\n",
      "State: (0, 1), Action: 1, Reward: 0\n",
      "State: (0, 2), Action: 1, Reward: 0\n",
      "State: (0, 3), Action: 1, Reward: 1\n",
      "Final State: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print the learned Q-values after training\n",
    "print(\"Learned Q-values:\")\n",
    "print(agent.q_table)\n",
    "# Test the learned policy\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.choose_action(state)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    state = next_state\n",
    "    print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n",
    "# Print the final state\n",
    "print(f\"Final State: {state}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
